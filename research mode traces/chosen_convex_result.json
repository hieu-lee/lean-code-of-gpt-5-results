{
    "statement": "Let $f:\\mathbb{R}^d\\to\\mathbb{R}$ be convex and differentiable. For the implicit (backward Euler) method $x_{n+1}=x_n-\\eta\\,\\nabla f(x_{n+1})$ with any $\\eta>0$, the sequence $n\\mapsto f(x_n)$ is convex: $\\delta_n\\ge 0$ for all $n$. Moreover, if $f$ is $\\mu$-strongly convex, then $(\\Delta_n)$ is strictly decreasing unless $x_n$ is optimal.",
    "proof_markdown": "**Claim.** Let $f:\\mathbb{R}^d\\to\\mathbb{R}$ be convex and differentiable. Consider the implicit (backward Euler / proximal point) scheme\n$$\n x_{n+1}=x_n-\\eta\\,\\nabla f(x_{n+1})\\qquad(\\eta>0),\n$$\nwith $a_n:=f(x_n)$, $\\Delta_n:=a_n-a_{n+1}$, and $\\delta_n:=a_{n+2}-2a_{n+1}+a_n=\\Delta_n-\\Delta_{n+1}$. Then $\\delta_n\\ge 0$ for all $n$. Moreover, if $f$ is $\\mu$-strongly convex, then $(\\Delta_n)$ is strictly decreasing unless $x_n$ is optimal.\n\n**Proof.** The optimality condition of the implicit step is\n$$\n\\nabla f(x_{n+1})+\\tfrac{1}{\\eta}(x_{n+1}-x_n)=0.\n$$\nSet $s_n:=x_{n+1}-x_n$. Then $\\nabla f(x_{n+1})=-\\tfrac{1}{\\eta}s_n$ and $x_{n+1}-x_{n+2}=-s_{n+1}$.\n\n1) By convexity, for all $u,v$,\n$$\n f(u)\\ge f(v)+\\langle\\nabla f(v),u-v\\rangle\\quad\\text{and}\\quad f(v)-f(u)\\le \\langle\\nabla f(v),v-u\\rangle.\n$$\nApplying these with $(u,v)=(x_n,x_{n+1})$ and $(u,v)=(x_{n+2},x_{n+1})$ gives\n$$\n \\Delta_n=f(x_n)-f(x_{n+1})\\ge \\langle\\nabla f(x_{n+1}),x_n-x_{n+1}\\rangle = \\tfrac{1}{\\eta}\\,\\|s_n\\|^2,\n$$\n$$\n \\Delta_{n+1}=f(x_{n+1})-f(x_{n+2})\\le \\langle\\nabla f(x_{n+1}),x_{n+1}-x_{n+2}\\rangle = \\tfrac{1}{\\eta}\\,\\langle s_n,s_{n+1}\\rangle.\n$$\nTherefore,\n$$\n \\delta_n=\\Delta_n-\\Delta_{n+1}\\ge \\tfrac{1}{\\eta}\\big(\\|s_n\\|^2-\\langle s_n,s_{n+1}\\rangle\\big)\n \\ge \\tfrac{1}{\\eta}\\,\\|s_n\\|\\big(\\|s_n\\|-\\|s_{n+1}\\|\\big),\n$$\nwhere the last step uses Cauchy–Schwarz.\n\n2) Since $f$ is convex, $\\nabla f$ is monotone, hence\n$$\n 0\\le \\langle\\nabla f(x_{n+1})-\\nabla f(x_{n+2}),x_{n+1}-x_{n+2}\\rangle \n = \\tfrac{1}{\\eta}\\langle s_n-s_{n+1},s_{n+1}\\rangle,\n$$\nso $\\langle s_n,s_{n+1}\\rangle\\ge \\|s_{n+1}\\|^2$. By Cauchy–Schwarz, $\\|s_{n+1}\\|\\le\\|s_n\\|$. Plugging this into the bound in (1) yields $\\delta_n\\ge 0$ for all $n$, i.e., $n\\mapsto f(x_n)$ is convex.\n\n3) If, in addition, $f$ is $\\mu$-strongly convex, then $\\nabla f$ is $\\mu$-strongly monotone:\n$$\n \\langle\\nabla f(u)-\\nabla f(v),u-v\\rangle\\ge \\mu\\,\\|u-v\\|^2\\quad(\\forall u,v).\n$$\nWith $u=x_{n+1}$ and $v=x_{n+2}$ we get\n$$\n \\tfrac{1}{\\eta}\\langle s_n-s_{n+1},s_{n+1}\\rangle\\ge \\mu\\,\\|s_{n+1}\\|^2\n \\;\\Rightarrow\\; \\langle s_n,s_{n+1}\\rangle\\ge (1+\\eta\\mu)\\,\\|s_{n+1}\\|^2.\n$$\nHence $\\|s_{n+1}\\|\\le \\tfrac{1}{1+\\eta\\mu}\\,\\|s_n\\|$, and if $s_n\\ne 0$ then $\\|s_{n+1}\\|<\\|s_n\\|$. Using (1),\n$$\n \\delta_n\\ge \\tfrac{1}{\\eta}\\,\\|s_n\\|\\big(\\|s_n\\|-\\|s_{n+1}\\|\\big)\n \\ge \\tfrac{\\mu}{1+\\eta\\mu}\\,\\|s_n\\|^2>0.\n$$\nThus $\\Delta_{n+1}<\\Delta_n$ strictly unless $s_n=0$, in which case the optimality condition gives $\\nabla f(x_n)=0$ and $x_n$ is optimal, yielding $\\Delta_n=\\Delta_{n+1}=0$.\n\nTherefore, $n\\mapsto f(x_n)$ is convex for all $\\eta>0$, and, under $\\mu$-strong convexity, $(\\Delta_n)$ is strictly decreasing unless $x_n$ is optimal. ∎"
}