We work in (\mathbb{R}^d,\langle\cdot,\cdot\rangle,\|\cdot\|). A function f:\mathbb{R}^d\to\mathbb{R} is convex and L-smooth if \nabla f is L-Lipschitz. Gradient descent (GD) with constant stepsize \eta>0 is x_{n+1}=x_n-\eta\nabla f(x_n). Gradient flow (GF) solves x'(t)=-\nabla f(x(t)). Write a_n:=f(x_n), \Delta_n:=a_n-a_{n+1}, and \delta_n:=a_{n+2}-2a_{n+1}+a_n. A sequence (a_n) is convex iff \delta_n\ge 0 for all n. For quadratics f(x)=\tfrac12 x^\top Qx+b^\top x+c with Q\succeq0, write g_n:=\nabla f(x_n). The extragradient (EG) step is x_{n+1}=x_n-\eta\nabla f(x_n-\eta\nabla f(x_n)). The implicit (backward Euler/proximal-point) step is x_{n+1}=\operatorname*{argmin}_x\{ f(x)+\tfrac{1}{2\eta}\|x-x_n\|^2\}, equivalently x_{n+1}=x_n-\eta\nabla f(x_{n+1}). The Moreau envelope is e_\gamma(f)(x):=\min_z\{ f(z)+\tfrac{1}{2\gamma}\|z-x\|^2\}. For a sequence (b_n), the 2-term moving average is M^{(\lambda)}(b)_n:=(1-\lambda)b_n+\lambda b_{n+1} with \lambda\in[0,1]. For nonexpansive mappings T, we use \|T(x)-T(y)\|\le\|x-y\|. We use \theta:=L\eta. Expectations are written \mathbb{E}[\cdot].