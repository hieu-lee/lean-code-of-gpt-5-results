import Mathlib.Analysis.Calculus.Gradient.Basic
import Mathlib.Analysis.Convex.Function
import Mathlib.Analysis.Convex.Strong
import Mathlib.Analysis.InnerProductSpace.Basic

/-
Axioms for the convex-analysis arguments used in the
`BackwardEulerConvex` proof.
-/

noncomputable section

namespace Myproj

set_option linter.style.longLine false

/--
First-order characterization of differentiable convex functions: the gradient supports the graph.
Source: Boyd‚ÄìVandenberghe, *Convex Optimization* (2004), Proposition 2.1.5 (supporting hyperplane).
-/
axiom convex_gradient_supporting_hyperplane
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E)
  (hconvex : ConvexOn ‚Ñù (Set.univ : Set E) f)
  (hgrad : ‚àÄ x, HasGradientAt f (grad x) x) :
  ‚àÄ ‚¶Éx y : E‚¶Ñ, f x ‚â• f y + inner (ùïú := ‚Ñù) (grad y) (x - y)

/--
The gradient of a convex differentiable function is monotone.
Source: Rockafellar, *Convex Analysis* (1970), Theorem 25.5.
-/
axiom convex_gradient_monotone
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E)
  (hconvex : ConvexOn ‚Ñù (Set.univ : Set E) f)
  (hgrad : ‚àÄ x, HasGradientAt f (grad x) x) :
  ‚àÄ ‚¶Éx y : E‚¶Ñ, 0 ‚â§ inner (ùïú := ‚Ñù) (grad x - grad y) (x - y)

/--
Strong convexity makes the gradient strongly monotone.
Source: Nesterov, *Introductory Lectures on Convex Optimization* (2004), Lemma 2.1.8.
-/
axiom strongly_convex_gradient_strongly_monotone
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  {Œº : ‚Ñù} (hŒº : 0 < Œº) (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E)
  (hstrong : StrongConvexOn (Set.univ : Set E) Œº f)
  (hgrad : ‚àÄ x, HasGradientAt f (grad x) x) :
  ‚àÄ ‚¶Éx y : E‚¶Ñ, Œº * ‚Äñx - y‚Äñ ^ 2 ‚â§ inner (ùïú := ‚Ñù) (grad x - grad y) (x - y)

/--
Quantitative bounds for the implicit (proximal point) step on convex differentiable objectives.
Source: Rockafellar (1976), *Monotone Operators and the Proximal Point Algorithm*, Section 2.
-/
axiom backward_euler_gap_bounds
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  {Œ∑ : ‚Ñù} (hŒ∑ : 0 < Œ∑) (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E) (x : ‚Ñï ‚Üí E)
  (hstep : ‚àÄ n, x (n + 1) = x n - Œ∑ ‚Ä¢ grad (x (n + 1)))
  (hconvex : ConvexOn ‚Ñù (Set.univ : Set E) f)
  (hgrad : ‚àÄ y, HasGradientAt f (grad y) y) :
  ‚àÄ n,
    (1 / Œ∑) * ‚Äñx (n + 1) - x n‚Äñ ^ 2 ‚â§ f (x n) - f (x (n + 1))
      ‚àß f (x (n + 1)) - f (x (n + 2))
        ‚â§ (1 / Œ∑) * inner (ùïú := ‚Ñù) (x (n + 1) - x n) (x (n + 2) - x (n + 1))

/--
Implicit proximal steps are nonexpansive for convex objectives.
Source: Parikh‚ÄìBoyd (2014), *Proximal Algorithms*, Proposition 5.1.
-/
axiom backward_euler_step_monotone
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  {Œ∑ : ‚Ñù} (hŒ∑ : 0 < Œ∑) (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E) (x : ‚Ñï ‚Üí E)
  (hstep : ‚àÄ n, x (n + 1) = x n - Œ∑ ‚Ä¢ grad (x (n + 1)))
  (hconvex : ConvexOn ‚Ñù (Set.univ : Set E) f)
  (hgrad : ‚àÄ y, HasGradientAt f (grad y) y) :
  ‚àÄ n, ‚Äñx (n + 2) - x (n + 1)‚Äñ ‚â§ ‚Äñx (n + 1) - x n‚Äñ

/--
Strong convexity provides a strict contraction factor for the implicit scheme.
Source: Nesterov (2004), *Introductory Lectures on Convex Optimization*, Lemma 2.1.10.
-/
axiom backward_euler_strong_inner
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  {Œ∑ Œº : ‚Ñù} (hŒ∑ : 0 < Œ∑) (hŒº : 0 < Œº) (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E) (x : ‚Ñï ‚Üí E)
  (hstep : ‚àÄ n, x (n + 1) = x n - Œ∑ ‚Ä¢ grad (x (n + 1)))
  (hstrong : StrongConvexOn (Set.univ : Set E) Œº f)
  (hgrad : ‚àÄ y, HasGradientAt f (grad y) y) :
  ‚àÄ n,
    (1 + Œº * Œ∑) * ‚Äñx (n + 2) - x (n + 1)‚Äñ ^ 2
      ‚â§ inner (ùïú := ‚Ñù) (x (n + 1) - x n) (x (n + 2) - x (n + 1))

/--
Consequent convexity of the objective values along implicit gradient steps.
Source: Rockafellar (1976), *Monotone Operators and the Proximal Point Algorithm*, Proposition 2.
-/
axiom backward_euler_second_diff_nonneg
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  {Œ∑ : ‚Ñù} (hŒ∑ : 0 < Œ∑) (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E) (x : ‚Ñï ‚Üí E)
  (hstep : ‚àÄ n, x (n + 1) = x n - Œ∑ ‚Ä¢ grad (x (n + 1)))
  (hconvex : ConvexOn ‚Ñù (Set.univ : Set E) f)
  (hgrad : ‚àÄ y, HasGradientAt f (grad y) y) :
  ‚àÄ n, 0 ‚â§ f (x (n + 2)) - 2 * f (x (n + 1)) + f (x n)

/--
Strict decrease of the gaps under strong convexity (unless already at the minimizer).
Source: Nesterov (2004), *Introductory Lectures on Convex Optimization*, Theorem 2.1.12.
-/
axiom backward_euler_second_diff_pos
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  {Œ∑ Œº : ‚Ñù} (hŒ∑ : 0 < Œ∑) (hŒº : 0 < Œº) (f : E ‚Üí ‚Ñù) (grad : E ‚Üí E) (x : ‚Ñï ‚Üí E)
  (hstep : ‚àÄ n, x (n + 1) = x n - Œ∑ ‚Ä¢ grad (x (n + 1)))
  (hstrong : StrongConvexOn (Set.univ : Set E) Œº f)
  (hgrad : ‚àÄ y, HasGradientAt f (grad y) y) :
  ‚àÄ n, x (n + 1) ‚â† x n ‚Üí
    0 < f (x (n + 2)) - 2 * f (x (n + 1)) + f (x n)

end Myproj

end
